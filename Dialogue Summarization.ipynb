{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3480fa63-f2b0-447a-a4e8-361499e2ee1a",
   "metadata": {},
   "source": [
    "# Summarize Dialogue with Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f6cea8-a18f-46a7-a256-c6b33a377737",
   "metadata": {},
   "source": [
    "#### credits: https://www.coursera.org/learn/generative-ai-with-llms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "330c1482-b83a-4487-b6e0-de4b598443aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (23.3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (2.11.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from datasets) (1.26.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from datasets) (0.20.1)\n",
      "Requirement already satisfied: packaging in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/a0u01ki/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.15.0)\n",
      "Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.11.0\n",
      "    Uninstalling datasets-2.11.0:\n",
      "      Successfully uninstalled datasets-2.11.0\n",
      "Successfully installed datasets-2.16.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch==1.13.1 \\\n",
    "    torchdata==0.5.1 --quiet\n",
    "\n",
    "%pip install \\\n",
    "    transformers==4.27.2 \\\n",
    "    datasets==2.11.0  --quiet\n",
    "%pip install datasets --upgrade\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7642f792-7144-4510-b53f-038e16b77212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <class 'numpy.int64'>\n",
      "1 <class 'numpy.int64'>\n",
      "2 <class 'numpy.int64'>\n",
      "3 <class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(np.arange(10,14)):\n",
    "    print(i, type(index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d74d409-d1b9-4b6b-bff4-8a49b2acd283",
   "metadata": {},
   "source": [
    "Load the datasets, Large Language Model (LLM), tokenizer, and configurator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ba6017a-c059-4593-8ba0-88b2733b1805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import GenerationConfig\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82837142-8960-4556-a5a3-2bbce41d0aa2",
   "metadata": {},
   "source": [
    "## Summarize diaglogue without Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "001a69d4-f937-4b1f-b59c-3e4136674b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|███████████████████████████████████████████████████████████████████████████████████████| 4.65k/4.65k [00:00<00:00, 4.94MB/s]\n",
      "Downloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 11.3M/11.3M [00:01<00:00, 6.77MB/s]\n",
      "Downloading data: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 442k/442k [00:00<00:00, 1.59MB/s]\n",
      "Downloading data: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 1.35M/1.35M [00:00<00:00, 2.79MB/s]\n",
      "Generating train split: 12460 examples [00:00, 81782.19 examples/s]\n",
      "Generating validation split: 500 examples [00:00, 88167.49 examples/s]\n",
      "Generating test split: 1500 examples [00:00, 151393.41 examples/s]\n"
     ]
    }
   ],
   "source": [
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(huggingface_dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9230e89f-efba-4c46-940d-8b911e11c4e1",
   "metadata": {},
   "source": [
    "Let's upload some simple dialogues from the DialogSum Hugging Face dataset. This dataset contains 10,000+ dialogues with the corresponding manually labeled summaries and topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e25c9d02-ca07-4765-8b28-b31d0985464a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "        num_rows: 12460\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Exploring the Dialogue sum dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488ad496-b3af-45d0-be88-5c0068abadb3",
   "metadata": {},
   "source": [
    "Lets print some dialogues and reference summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a53b6782-a686-49d9-b17d-297fb7135488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example 1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT_DIALOGUE\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY\n",
      "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example 2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT_DIALOGUE\n",
      "#Person1#: This Olympic park is so big!\n",
      "#Person2#: Yes. Now we are in the Olympic stadium, the center of this park.\n",
      "#Person1#: Splendid! When is it gonna be finished?\n",
      "#Person2#: The whole stadium is to be finished this June.\n",
      "#Person1#: How many seats are there in the stand?\n",
      "#Person2#: Oh, there are 5000 seats in total.\n",
      "#Person1#: I didn ' t know it would be so big!\n",
      "#Person2#: It is! Look there, those are the tracks. And the jumping pit is over there.\n",
      "#Person1#: Ah... I see. Hey, look the sign here, No climbing.\n",
      "#Person2#: We put many signs with English translations for foreign visitors.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY\n",
      "#Person1# is surprised at the Olympic Stadium'volume, capacity and interior setting to #Person1#.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example 3\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT_DIALOGUE\n",
      "#Person1#: What's wrong with you? Why are you scratching so much?\n",
      "#Person2#: I feel itchy! I can't stand it anymore! I think I may be coming down with something. I feel lightheaded and weak.\n",
      "#Person1#: Let me have a look. Whoa! Get away from me!\n",
      "#Person2#: What's wrong?\n",
      "#Person1#: I think you have chicken pox! You are contagious! Get away! Don't breathe on me!\n",
      "#Person2#: Maybe it's just a rash or an allergy! We can't be sure until I see a doctor.\n",
      "#Person1#: Well in the meantime you are a biohazard! I didn't get it when I was a kid and I've heard that you can even die if you get it as an adult!\n",
      "#Person2#: Are you serious? You always blow things out of proportion. In any case, I think I'll go take an oatmeal bath.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY\n",
      "#Person2# feels itchy. #Person1# doubts it is chicken pox and asks #Person2# to get away. #Person2# doesn't believe it.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_indices = [10, 12, 18]\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(example_indices):\n",
    "    print(dash_line)\n",
    "    print(\"Example\", i + 1)\n",
    "    print(dash_line)\n",
    "    print('INPUT_DIALOGUE')\n",
    "    print(dataset['test'][index]['dialogue']) # dialogue \n",
    "    print(dash_line)\n",
    "    print('BASELINE HUMAN SUMMARY')\n",
    "    print(dataset['test'][index]['summary']) # summary\n",
    "    print(dash_line)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7b6931-d555-4fb9-a032-44139b95f505",
   "metadata": {},
   "source": [
    "Loding a pretrained multi-taks FLan T5 language model. \n",
    "FLAN-T5 was released in the paper Scaling Instruction-Finetuned Language Models - it is an enhanced version of T5 that has been finetuned in a mixture of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99b49577-860a-4bff-b77f-3d1f6ddcf420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1.40k/1.40k [00:00<00:00, 399kB/s]\n",
      "model.safetensors: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 990M/990M [02:30<00:00, 6.59MB/s]\n",
      "generation_config.json: 100%|███████████████████████████████████████████████████████████████████████████████████████| 147/147 [00:00<00:00, 68.9kB/s]\n"
     ]
    }
   ],
   "source": [
    "model_name = 'google/flan-t5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2467cb4-ae8b-4e8f-8b01-eb9367ce06d2",
   "metadata": {},
   "source": [
    "To perform encoding and decoding, you need to work with text in a tokenized form. Tokenization is the process of splitting texts into smaller units that can be processed by the LLM models.\n",
    "\n",
    "It is important to use the compatible tokenizer for the language model\n",
    "\n",
    "Download the tokenizer for the FLAN-T5 model using AutoTokenizer.from_pretrained() method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3493528e-fcfe-4a6e-bf78-7e1dec88a6ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|████████████████████████████████████████████████████████████████████████████████████| 2.54k/2.54k [00:00<00:00, 1.25MB/s]\n",
      "spiece.model: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 792k/792k [00:00<00:00, 4.29MB/s]\n",
      "tokenizer.json: 100%|███████████████████████████████████████████████████████████████████████████████████████████| 2.42M/2.42M [00:00<00:00, 8.91MB/s]\n",
      "special_tokens_map.json: 100%|███████████████████████████████████████████████████████████████████████████████████| 2.20k/2.20k [00:00<00:00, 987kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1977f7d5-41cf-444c-80b0-006df66a43d9",
   "metadata": {},
   "source": [
    "Test the tokenizer encoding and decoding a simple sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29547fcf-c6a1-4f24-99d9-8c3c934c2d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 363,   97,   19,   34,    6, 3059,   58,    1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"What time is it, Tom?\"\n",
    "\n",
    "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
    "sentence_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d5352f0-1a66-4f24-aa60-df73d0585309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What time is it, Tom?</s>'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_decoded = tokenizer.decode(\n",
    "        sentence_encoded[\"input_ids\"][0]\n",
    "    )\n",
    "sentence_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5516388-b9a7-424d-99ea-bafbe7909aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED SENTENCE:\n",
      "tensor([ 363,   97,   19,   34,    6, 3059,   58,    1])\n",
      "\n",
      "DECODED SENTENCE:\n",
      "What time is it, Tom?\n"
     ]
    }
   ],
   "source": [
    "sentence = \"What time is it, Tom?\"\n",
    "\n",
    "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "sentence_decoded = tokenizer.decode(\n",
    "        sentence_encoded[\"input_ids\"][0], \n",
    "        skip_special_tokens=True  # to skip special EOS tokens\n",
    "    )\n",
    "\n",
    "print('ENCODED SENTENCE:')\n",
    "print(sentence_encoded[\"input_ids\"][0])\n",
    "print('\\nDECODED SENTENCE:')\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76336ef7-4179-4789-95e6-c1af6c676a3a",
   "metadata": {},
   "source": [
    "Now it's time to explore how well the base LLM summarizes a dialogue without any prompt engineering.\n",
    "Prompt engineering is an act of a human changing the prompt (input) to improve the response for a given task. Idea was inpired from Zero shot learning of LLM models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "65e53266-cc55-48ea-8eae-a44ce4c1e95b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION WITHOUT Prompt engineering:\n",
      "Brian, thank you for coming to our party.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: This Olympic park is so big!\n",
      "#Person2#: Yes. Now we are in the Olympic stadium, the center of this park.\n",
      "#Person1#: Splendid! When is it gonna be finished?\n",
      "#Person2#: The whole stadium is to be finished this June.\n",
      "#Person1#: How many seats are there in the stand?\n",
      "#Person2#: Oh, there are 5000 seats in total.\n",
      "#Person1#: I didn ' t know it would be so big!\n",
      "#Person2#: It is! Look there, those are the tracks. And the jumping pit is over there.\n",
      "#Person1#: Ah... I see. Hey, look the sign here, No climbing.\n",
      "#Person2#: We put many signs with English translations for foreign visitors.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is surprised at the Olympic Stadium'volume, capacity and interior setting to #Person1#.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION WITHOUT Prompt engineering:\n",
      "#Person1#: The Olympic stadium is so big. #Person2#: Oh, it is! There are 5000 seats in the stand. #Person1#: Wow, that's big! #Person2\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  3\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: What's wrong with you? Why are you scratching so much?\n",
      "#Person2#: I feel itchy! I can't stand it anymore! I think I may be coming down with something. I feel lightheaded and weak.\n",
      "#Person1#: Let me have a look. Whoa! Get away from me!\n",
      "#Person2#: What's wrong?\n",
      "#Person1#: I think you have chicken pox! You are contagious! Get away! Don't breathe on me!\n",
      "#Person2#: Maybe it's just a rash or an allergy! We can't be sure until I see a doctor.\n",
      "#Person1#: Well in the meantime you are a biohazard! I didn't get it when I was a kid and I've heard that you can even die if you get it as an adult!\n",
      "#Person2#: Are you serious? You always blow things out of proportion. In any case, I think I'll go take an oatmeal bath.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2# feels itchy. #Person1# doubts it is chicken pox and asks #Person2# to get away. #Person2# doesn't believe it.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION WITHOUT Prompt engineering:\n",
      "Person1#: I'm scratching so much. I can't stand it anymore. I think I may be coming down with something. I feel lightheaded and weak.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    inputs = tokenizer(dialogue, return_tensors='pt')\n",
    "    output = tokenizer.decode(model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0], skip_special_tokens=True)\n",
    "\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION WITHOUT Prompt engineering:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256db55c-c416-4dc8-8b22-dd0afe38feaa",
   "metadata": {},
   "source": [
    "Model is output makes sense but it doesn't seem to know what the task we are doign to accomplish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77348483-f305-4908-9d92-299b5030d14c",
   "metadata": {},
   "source": [
    "## Summarize dialogue with Instruction Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac2a2f-2dd8-4bb7-8423-9a65f7479f9b",
   "metadata": {},
   "source": [
    "we are going to use promt engineering to make summaries aligned more to our task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b179674e-dddd-4b8c-a45a-14b2d17b875b",
   "metadata": {},
   "source": [
    "### Zero Shot inference with Instruction Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0787481c-7c42-4a2a-8230-52c08dd7aeed",
   "metadata": {},
   "source": [
    "#### Wrapping the dialogue with a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b213a7f3-cca1-450b-a2ee-ebefc69564d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION WITH Prompt engineering:\n",
      "Brian's birthday is coming up.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: This Olympic park is so big!\n",
      "#Person2#: Yes. Now we are in the Olympic stadium, the center of this park.\n",
      "#Person1#: Splendid! When is it gonna be finished?\n",
      "#Person2#: The whole stadium is to be finished this June.\n",
      "#Person1#: How many seats are there in the stand?\n",
      "#Person2#: Oh, there are 5000 seats in total.\n",
      "#Person1#: I didn ' t know it would be so big!\n",
      "#Person2#: It is! Look there, those are the tracks. And the jumping pit is over there.\n",
      "#Person1#: Ah... I see. Hey, look the sign here, No climbing.\n",
      "#Person2#: We put many signs with English translations for foreign visitors.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is surprised at the Olympic Stadium'volume, capacity and interior setting to #Person1#.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION WITH Prompt engineering:\n",
      "The Olympic stadium is the center of the park.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  3\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: What's wrong with you? Why are you scratching so much?\n",
      "#Person2#: I feel itchy! I can't stand it anymore! I think I may be coming down with something. I feel lightheaded and weak.\n",
      "#Person1#: Let me have a look. Whoa! Get away from me!\n",
      "#Person2#: What's wrong?\n",
      "#Person1#: I think you have chicken pox! You are contagious! Get away! Don't breathe on me!\n",
      "#Person2#: Maybe it's just a rash or an allergy! We can't be sure until I see a doctor.\n",
      "#Person1#: Well in the meantime you are a biohazard! I didn't get it when I was a kid and I've heard that you can even die if you get it as an adult!\n",
      "#Person2#: Are you serious? You always blow things out of proportion. In any case, I think I'll go take an oatmeal bath.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2# feels itchy. #Person1# doubts it is chicken pox and asks #Person2# to get away. #Person2# doesn't believe it.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION WITH Prompt engineering:\n",
      "Person1 is scratching a lot.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversations\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "    # Input constructed on dialogue instead of dialogue\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0], skip_special_tokens=True)\n",
    "\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION WITH Prompt engineering:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189758f8-6438-48a7-a703-8db4f4afa2de",
   "metadata": {},
   "source": [
    "This is much better but model doesn't pick up the nuances of the conversation though"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a11c8d-c49f-4f45-8c92-f34c7552efc1",
   "metadata": {},
   "source": [
    "Changing the prompt also has an effect on the model generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e196b6bb-7122-4f35-bb4a-f5a526e3e968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION WITH Prompt engineering:\n",
      "Brian's birthday is coming up.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: This Olympic park is so big!\n",
      "#Person2#: Yes. Now we are in the Olympic stadium, the center of this park.\n",
      "#Person1#: Splendid! When is it gonna be finished?\n",
      "#Person2#: The whole stadium is to be finished this June.\n",
      "#Person1#: How many seats are there in the stand?\n",
      "#Person2#: Oh, there are 5000 seats in total.\n",
      "#Person1#: I didn ' t know it would be so big!\n",
      "#Person2#: It is! Look there, those are the tracks. And the jumping pit is over there.\n",
      "#Person1#: Ah... I see. Hey, look the sign here, No climbing.\n",
      "#Person2#: We put many signs with English translations for foreign visitors.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is surprised at the Olympic Stadium'volume, capacity and interior setting to #Person1#.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION WITH Prompt engineering:\n",
      "The Olympic park is a huge stadium.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  3\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: What's wrong with you? Why are you scratching so much?\n",
      "#Person2#: I feel itchy! I can't stand it anymore! I think I may be coming down with something. I feel lightheaded and weak.\n",
      "#Person1#: Let me have a look. Whoa! Get away from me!\n",
      "#Person2#: What's wrong?\n",
      "#Person1#: I think you have chicken pox! You are contagious! Get away! Don't breathe on me!\n",
      "#Person2#: Maybe it's just a rash or an allergy! We can't be sure until I see a doctor.\n",
      "#Person1#: Well in the meantime you are a biohazard! I didn't get it when I was a kid and I've heard that you can even die if you get it as an adult!\n",
      "#Person2#: Are you serious? You always blow things out of proportion. In any case, I think I'll go take an oatmeal bath.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2# feels itchy. #Person1# doubts it is chicken pox and asks #Person2# to get away. #Person2# doesn't believe it.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION WITH Prompt engineering:\n",
      "Person1 is scratching so much that he can't stand it anymore. He's allergic to chicken pox. He's also feeling lightheaded and weak.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# changing the prompt\n",
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "brief of conversations\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "    # Input constructed on dialogue instead of dialogue\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0], skip_special_tokens=True)\n",
    "\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION WITH Prompt engineering:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e4b2b4-aa54-41be-aaa8-98908180f89e",
   "metadata": {},
   "source": [
    "the model summary for the 3rd example seems to change if we change the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70325869-9b42-4907-a1b3-5f3fa200e186",
   "metadata": {},
   "source": [
    "### Zero shot inference with Prmopt templated from Flan-T5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad7ba7a-7545-4409-b0db-f32f4e896b1e",
   "metadata": {},
   "source": [
    "Flan-T5 has many promot temaplated published for certain tasks. We will use one of the pre-build FLAN T5 promts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d4862b0-cf62-43b7-8984-a518a9ae7309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: Happy Birthday, this is for you, Brian.\n",
      "#Person2#: I'm so happy you remember, please come in and enjoy the party. Everyone's here, I'm sure you have a good time.\n",
      "#Person1#: Brian, may I have a pleasure to have a dance with you?\n",
      "#Person2#: Ok.\n",
      "#Person1#: This is really wonderful party.\n",
      "#Person2#: Yes, you are always popular with everyone. and you look very pretty today.\n",
      "#Person1#: Thanks, that's very kind of you to say. I hope my necklace goes with my dress, and they both make me look good I feel.\n",
      "#Person2#: You look great, you are absolutely glowing.\n",
      "#Person1#: Thanks, this is a fine party. We should have a drink together to celebrate your birthday\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# attends Brian's birthday party. Brian thinks #Person1# looks great and charming.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION WITH Prompt engineering:\n",
      "Brian's birthday is coming up.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: This Olympic park is so big!\n",
      "#Person2#: Yes. Now we are in the Olympic stadium, the center of this park.\n",
      "#Person1#: Splendid! When is it gonna be finished?\n",
      "#Person2#: The whole stadium is to be finished this June.\n",
      "#Person1#: How many seats are there in the stand?\n",
      "#Person2#: Oh, there are 5000 seats in total.\n",
      "#Person1#: I didn ' t know it would be so big!\n",
      "#Person2#: It is! Look there, those are the tracks. And the jumping pit is over there.\n",
      "#Person1#: Ah... I see. Hey, look the sign here, No climbing.\n",
      "#Person2#: We put many signs with English translations for foreign visitors.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# is surprised at the Olympic Stadium'volume, capacity and interior setting to #Person1#.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION WITH Prompt engineering:\n",
      "The Olympic stadium is to be finished this June. There are 5000 seats in the stand.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example  3\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "#Person1#: What's wrong with you? Why are you scratching so much?\n",
      "#Person2#: I feel itchy! I can't stand it anymore! I think I may be coming down with something. I feel lightheaded and weak.\n",
      "#Person1#: Let me have a look. Whoa! Get away from me!\n",
      "#Person2#: What's wrong?\n",
      "#Person1#: I think you have chicken pox! You are contagious! Get away! Don't breathe on me!\n",
      "#Person2#: Maybe it's just a rash or an allergy! We can't be sure until I see a doctor.\n",
      "#Person1#: Well in the meantime you are a biohazard! I didn't get it when I was a kid and I've heard that you can even die if you get it as an adult!\n",
      "#Person2#: Are you serious? You always blow things out of proportion. In any case, I think I'll go take an oatmeal bath.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person2# feels itchy. #Person1# doubts it is chicken pox and asks #Person2# to get away. #Person2# doesn't believe it.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION WITH Prompt engineering:\n",
      "Person1 is scratching so much that he can't stand it anymore. He feels lightheaded and weak.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(example_indices):\n",
    "    dialogue = dataset['test'][index]['dialogue']\n",
    "    summary = dataset['test'][index]['summary']\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "    \"\"\"\n",
    "    # Input constructed on dialogue instead of dialogue\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    output = tokenizer.decode(model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0], skip_special_tokens=True)\n",
    "\n",
    "    print(dash_line)\n",
    "    print('Example ', i + 1)\n",
    "    print(dash_line)\n",
    "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
    "    print(dash_line)\n",
    "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "    print(dash_line)\n",
    "    print(f'MODEL GENERATION WITH Prompt engineering:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43f54da-6e30-4496-a3f6-54ac9f273243",
   "metadata": {},
   "source": [
    "Notice that this prompt from FLAN-T5 did help a bit, but still struggles to pick up on the nuance of the conversation. Now we will try with few shot inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e25dc0-380a-4f92-9f80-c93f3dbe57dd",
   "metadata": {},
   "source": [
    "## Summarize with one shot and few shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692ec64d-fc19-4fc7-949f-eeac97e9492a",
   "metadata": {},
   "source": [
    "One shot and few shot inference are the practices of providing an LLM with either one or more full examples of prompt-response pairs that match your task - before your actual prompt that you want completed. This is called \"in-context learning\" and puts your model into a state that understands your specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818db9d6-fe45-42ba-88d8-ef36f7e1fafa",
   "metadata": {},
   "source": [
    "### One shot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fb71e5c6-017c-4e42-b3ba-a7a9bd037af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(example_indices_full, example_index_to_summarize):\n",
    "    prompt = ''\n",
    "    for index in (example_indices_full):\n",
    "        dialogue = dataset['test'][index]['dialogue']\n",
    "        summary = dataset['test'][index]['summary']\n",
    "\n",
    "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n",
    "        prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "{summary}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
    "\n",
    "    prompt += f\"\"\"\n",
    "Dialogue:\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "What was going on?\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3079bec2-92fd-4c81-8ae1-a45a9b60b181",
   "metadata": {},
   "source": [
    "Construct prompt with one shot inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "43122869-1218-44e3-b99a-a06df272cdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "What was going on?\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "What was going on?\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [40]\n",
    "example_index_to_summarize = 200\n",
    "\n",
    "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "71ac4086-0a38-4abf-b2a6-ff4aed7618b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION ONE SHOT:\n",
      "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to add a CD-ROM drive.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "# Input constructed on dialogue instead of dialogue\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "output = tokenizer.decode(model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION ONE SHOT:\\n{output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca2cb65-83c2-4158-958b-b72d92984efb",
   "metadata": {},
   "source": [
    "## Few Shot inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ba0a2c-e2ba-4890-b696-68783ea6ebe6",
   "metadata": {},
   "source": [
    "Exploring with more example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "508b1343-4023-4eac-8f95-c704e6add4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "\n",
      "What was going on?\n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: May, do you mind helping me prepare for the picnic?\n",
      "#Person2#: Sure. Have you checked the weather report?\n",
      "#Person1#: Yes. It says it will be sunny all day. No sign of rain at all. This is your father's favorite sausage. Sandwiches for you and Daniel.\n",
      "#Person2#: No, thanks Mom. I'd like some toast and chicken wings.\n",
      "#Person1#: Okay. Please take some fruit salad and crackers for me.\n",
      "#Person2#: Done. Oh, don't forget to take napkins disposable plates, cups and picnic blanket.\n",
      "#Person1#: All set. May, can you help me take all these things to the living room?\n",
      "#Person2#: Yes, madam.\n",
      "#Person1#: Ask Daniel to give you a hand?\n",
      "#Person2#: No, mom, I can manage it by myself. His help just causes more trouble.\n",
      "\n",
      "What was going on?\n",
      "Mom asks May to help to prepare for the picnic and May agrees.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Hello, I bought the pendant in your shop, just before. \n",
      "#Person2#: Yes. Thank you very much. \n",
      "#Person1#: Now I come back to the hotel and try to show it to my friend, the pendant is broken, I'm afraid. \n",
      "#Person2#: Oh, is it? \n",
      "#Person1#: Would you change it to a new one? \n",
      "#Person2#: Yes, certainly. You have the receipt? \n",
      "#Person1#: Yes, I do. \n",
      "#Person2#: Then would you kindly come to our shop with the receipt by 10 o'clock? We will replace it. \n",
      "#Person1#: Thank you so much. \n",
      "\n",
      "What was going on?\n",
      "#Person1# wants to change the broken pendant in #Person2#'s shop.\n",
      "\n",
      "\n",
      "\n",
      "Dialogue:\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "What was going on?\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "example_indices_full = [40, 80, 120]\n",
    "example_indices_to_summarize = 200\n",
    "\n",
    "few_shot_prompts = make_prompt(example_indices_full, example_index_to_summarize)\n",
    "\n",
    "print(few_shot_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "848136c7-5b6d-4e6b-b494-a000ccbc3a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION ONE SHOT:\n",
      "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to add a CD-ROM drive.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION FEW SHOT:\n",
      "#Person1 wants to upgrade his system. #Person2 wants to add a painting program to his software. #Person1 wants to upgrade his hardware.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary = dataset['test'][example_index_to_summarize]['summary']\n",
    "\n",
    "# Input constructed on dialogue instead of dialogue\n",
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "output_one_shot = tokenizer.decode(model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0], skip_special_tokens=True)\n",
    "\n",
    "inputs = tokenizer(few_shot_prompts, return_tensors='pt')\n",
    "output_few_shot = tokenizer.decode(model.generate(inputs[\"input_ids\"], max_new_tokens=50)[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION ONE SHOT:\\n{output_one_shot}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION FEW SHOT:\\n{output_few_shot}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69658f80-7878-4a5b-961b-141bb3e42274",
   "metadata": {},
   "source": [
    "In this case, few shot did not provide much of an improvement over one shot inference. And, anything above 5 or 6 shot will typically not help much, either. Also, you need to make sure that you do not exceed the model's input-context length which, in our case, if 512 tokens. Anything above the context length will be ignored.\n",
    "\n",
    "However, you can see that feeding in at least one full example (one shot) provides the model with more information and qualitatively improves the summary overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d577d5e-41ac-447f-a21e-48e7e6d8a42f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
